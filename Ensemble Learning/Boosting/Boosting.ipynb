{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting, also known as hypothesis boosting, is an ensemble learning technique that combines multiple weak learners to form a strong learner. The key idea behind boosting is to train predictors sequentially, where each new predictor attempts to correct the errors of its predecessor.\n",
    "\n",
    "Three of the most popular boosting methods are:\n",
    "\n",
    "1) **AdaBoost (Adaptive Boosting):**\n",
    "    - Focuses on assigning higher weights to misclassified samples, so subsequent models can pay more attention to these samples.\n",
    "    - Works well for binary and multi-class classification problems.\n",
    "\n",
    "2) **Gradient Boosting:**\n",
    "    - Builds models sequentially, where each model tries to minimize the loss function of the ensemble using gradient descent techniques.\n",
    "    - Commonly used with decision trees for both classification and regression tasks.\n",
    "\n",
    "3) **XGBoost (Extreme Gradient Boosting):**\n",
    "    - An advanced implementation of gradient boosting with optimizations for speed and performance.\n",
    "    - Includes features like regularization, parallel processing, and handling missing values, making it efficient and robust for large datasets.\n",
    "    - Widely used in machine learning competitions and real-world applications due to its superior accuracy and scalability."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
